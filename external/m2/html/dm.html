

<!DOCTYPE html>


<html lang="fr" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>DM - Data Mining &#8212; Documentation M2 courses recap </title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dm';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="next" title="BIML - Bio-Inspired Machine Learning" href="biml.html" />
    <link rel="prev" title="TAA - Techniques d’apprentissage automatique" href="taa.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    <p class="title logo__title">Documentation M2 courses recap </p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="aic.html">AIC - Artificial Intelligence and Cognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="ccg.html">CCG - Combinatoire, Complexité et Graphes</a></li>
<li class="toctree-l1"><a class="reference internal" href="taa.html">TAA - Techniques d’apprentissage automatique</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">DM - Data Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="biml.html">BIML - Bio-Inspired Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ia.html">IA - intelligent Agents</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Téléchargez cette page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dm.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Télécharger le fichier source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Imprimer au format PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DM - Data Mining</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-data-description">Introduction, Data Description</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-de-donnees">Types de données</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decrire-une-variable">Décrire une variable</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value">P-value</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deviation-standard">Déviation standard</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions-de-variables">Interactions de variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrice-de-covariance">Matrice de covariance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-de-correlation">Coefficient de corrélation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-de-spearman">Corrélation de Spearman</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notions-de-distances">Notions de distances</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature scaling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regles-d-or">Règles d’or</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-beyond-k-means">Clustering beyond k-means</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means">K-means</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#melanges-gaussien-gaussian-mixtures">Mélanges Gaussien (Gaussian mixtures)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan">DBSCAN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-de-clustering">Evaluation de clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#score-silhouette">Score silhouette</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#davies-bouldin-index-dbi">Davies-Bouldin Index (DBI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dunn-index">Dunn Index</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-i-centralities">Networks I - Centralities</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#centralitees">Centralitées</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions-recursives">Définitions récursives</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank">PageRank</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-ii-detection-de-communautes">Networks II - Détection de communautés</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quelques-algos">Quelques algos</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#girvan-newman">Girvan &amp; Newman</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#louvain">Louvain</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#infomap">Infomap</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-block-model-sbm">Stochastic Block Model (SBM)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-de-structure-de-communautes">Evaluation de structure de communautés</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-de-liens">Prédiction de liens</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#premiere-approche-non-supervise-heuristique">Première approche (Non-supervisé) - Heuristique</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#seconde-approche-supervise-score-de-similarite">Seconde approche (Supervisé) - Score de similarité</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-de-noeuds">Classification de noeuds</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#methodes-recentes">Méthodes récentes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendation-factorisation-matricielle">Recommendation / Factorisation matricielle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-du-modele">Définition du modèle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-based-knn">User-based KNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#item-based-collaborative-filtering">Item-based Collaborative Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization-collaborative-filtering">Matrix Factorization Collaborative Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-des-systemes-de-recommandations">Evaluation des systèmes de recommandations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variante-de-mf-non-negative-matrix-factorization-nmf">Variante de MF - Non-negative Matrix Factorization (NMF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#co-clustering">Co-Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-de-dimension-low-dimensionality-embedding">Reduction de dimension <em>Low dimensionality embedding</em></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manifolds">Manifolds</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mds">MDS</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#isomap">Isomap</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-sne">T-SNE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-dimensionality-embedding">Low dimensionality embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding">Word Embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-embedding">Graph Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deepwalk">Deepwalk</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roles-des-embeddings">Rôles des embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-and-embeddings">Deep Learning and embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objects-vectors-to-graphs">Objects/Vectors to Graphs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequent-pattern-mining">Frequent Pattern Mining</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scores-d-interet">Scores d’intérêt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequent-itemset-extraction">Frequent Itemset Extraction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme-apriori">Algorithme Apriori</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#une-question-de-recherche-communities-in-degenerate-link-streams">Une question de recherche : Communities in degenerate link streams</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slowly-evolving-networks-sen">Slowly evolving networks (SEN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unstable-degenerate-temporal-networks">Unstable/Degenerate temporal networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#centralites-propriete-d-un-reseau-dans-un-stream-graph">Centralités &amp; Propriété d’un réseau dans un Stream Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-models-for-dynamix-networks">Random Models for dynamix networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-community-detection">Dynamic Community Detection</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="dm-data-mining">
<h1>DM - Data Mining<a class="headerlink" href="#dm-data-mining" title="Lien permanent vers cette rubrique">#</a></h1>
<div class="line-block">
<div class="line">Dispensé par : <em>Rémy Cazabet</em> (2023 - Sept.Nov)</div>
</div>
<section id="introduction-data-description">
<h2>Introduction, Data Description<a class="headerlink" href="#introduction-data-description" title="Lien permanent vers cette rubrique">#</a></h2>
<section id="types-de-donnees">
<h3>Types de données<a class="headerlink" href="#types-de-donnees" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p><strong>Nomimal</strong> : Pas d’ordre à priori.</p></li>
<li><p><strong>Ordinal</strong> : Peuvent être ordonnées.</p></li>
<li><p><strong>Interval</strong> : Intervalles de valeurs numériques (ex: température 30°-20° = 15°-5°).</p></li>
<li><p><strong>Ratio</strong> : Opérations sur des valeurs numériques.</p></li>
</ul>
<div class="line-block">
<div class="line">Dans la vraie vie, c’est plus compliqué. Il faut faire des choix de modélisations.</div>
<div class="line">Il y a des pièges : latitude/longitude, date, etc.</div>
<div class="line">Il manque souvent des valeurs dans les datasets. Ou les données peuvent être erronées.</div>
</div>
<aside class="topic">
<p class="topic-title">2 problèmes communs</p>
<ul class="simple">
<li><p>Valeurs out-of-range (ex: un poids de personne négatif).</p></li>
<li><p>Zeros (ex: un poids de personne de 0kg).</p></li>
</ul>
</aside>
<div class="line-block">
<div class="line">Une seule feature <span class="math notranslate nohighlight">\(\rightarrow\)</span> univariate (ex: Age).</div>
<div class="line">Dans la vraie vie <span class="math notranslate nohighlight">\(\rightarrow\)</span> multivariate (ex: (Age, Sexe, Poids, Taille, etc.)).</div>
</div>
</section>
<section id="decrire-une-variable">
<h3>Décrire une variable<a class="headerlink" href="#decrire-une-variable" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Moyenne</p></li>
<li><p>Médiane</p></li>
<li><p>Mode</p></li>
<li><p>Min/Max</p></li>
<li><p>…</p></li>
</ul>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">Une <strong>distribution</strong> est une déscription de la fréquence à laquelle on retrouve des items.</div>
<div class="line">C’est une fonction générée décrivant la possibilité d’observer un évènement possible.</div>
<div class="line">Elle peut être <strong>discrète</strong> ou <strong>continue</strong>.</div>
</div>
</div>
<aside class="topic">
<p class="topic-title">Distribution normale</p>
<div class="line-block">
<div class="line">Beaucoup de problèmes réels suivent une distribution normale.</div>
<div class="line">Variations aléatoires autour d’une moyenne définie.</div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/dm-distribution.png"><img alt="_images/dm-distribution.png" src="_images/dm-distribution.png" style="width: 388.0px; height: 245.0px;" /></a>
</figure>
</aside>
<aside class="topic">
<p class="topic-title">Distribution de la loi puissance</p>
<div class="line-block">
<div class="line">Un changement relatif sur une quantité résulte en un changement proportionnellement relatif sur une autre quantité.</div>
<div class="line">Ex: Les tremblements de terre 10 fois plus puissants sont <span class="math notranslate nohighlight">\(x\)</span> fois moins fréquents.</div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/dm-lawpower.png"><img alt="_images/dm-lawpower.png" src="_images/dm-lawpower.png" style="width: 329.0px; height: 178.0px;" /></a>
</figure>
</aside>
<section id="p-value">
<h4>P-value<a class="headerlink" href="#p-value" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">La <strong>p-value</strong> est la probabilité que ma donnée observée soit observable sur une distribution générée.</div>
<div class="line">Une p-value élevée a de grande probabilité de venir d’une hypothèse nulle.</div>
</div>
</div>
<div class="line-block">
<div class="line">On met un <em>threshold</em> sur la p-value (comme 0.05).</div>
<div class="line">Si la p-value est inférieure au <em>threshold</em>, on peut dire que la donnée a peu de chance que la donnée observée soit observable sur une distribution générée.</div>
<div class="line">Si elle est au-dessus, on peut dire que c’est probable.</div>
</div>
</section>
<section id="variance">
<h4>Variance<a class="headerlink" href="#variance" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Moyenne des carrés des écarts à la moyenne.</div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}Var(X)=\sigma^{2}=E[(X-\mu)^{2}] \\
\sigma^{2}={\frac {1}{N^{2}}\sum _{i&lt;j}(x_{i}-x_{j})^{2}\end{split}\]</div>
</section>
<section id="deviation-standard">
<h4>Déviation standard<a class="headerlink" href="#deviation-standard" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Racine carrée de la variance.</div>
</div>
<div class="math notranslate nohighlight">
\[\sigma =\sqrt{\sigma ^{2}}=\sqrt{E[(X-\mu )^{2}]}\]</div>
</section>
</section>
<section id="interactions-de-variables">
<h3>Interactions de variables<a class="headerlink" href="#interactions-de-variables" title="Lien permanent vers cette rubrique">#</a></h3>
<section id="matrice-de-covariance">
<h4>Matrice de covariance<a class="headerlink" href="#matrice-de-covariance" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Extension de la variance à plusieurs variables.</div>
<div class="line">La covariance est dure à interpréter. Si elle est supérieur à 0, il y a un lien de corrélation.</div>
<div class="line">On la normalise pour obtenir le coefficient de corrélation.</div>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\begin{bmatrix}
Var(X_{1}) &amp; Cov(X_{1},X_{2}) &amp; \cdots &amp; Cov(X_{1},X_{n}) \\
Cov(X_{2},X_{1}) &amp; Var(X_{2}) &amp; \cdots &amp; Cov(X_{2},X_{n}) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Cov(X_{n},X_{1}) &amp; Cov(X_{n},X_{2}) &amp; \cdots &amp; Var(X_{n})
\end{bmatrix}\end{split}\\\begin{split}Cov(X,Y)=E[(X-E[X])(Y-E[Y])] \\
Cov(X,X)=Var(X)\end{split}\end{aligned}\end{align} \]</div>
</section>
<section id="coefficient-de-correlation">
<h4>Coefficient de corrélation<a class="headerlink" href="#coefficient-de-correlation" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Coefficient de corrélation de Pearson.</div>
</div>
<ul>
<li><p>Normaliser la covariance par la déviation standard.</p></li>
<li><p>Pas besoin d’avoir des données normalisées.</p></li>
<li><div class="line-block">
<div class="line"><strong>+1</strong> : corrélation positive parfaite <span class="math notranslate nohighlight">\(X=aY\)</span>.</div>
<div class="line"><strong>-1</strong> : corrélation négative parfaite <span class="math notranslate nohighlight">\(X=-bY\)</span>.</div>
<div class="line"><strong>0</strong> : On ne peut rien dire.</div>
</div>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\rho_{X,Y}={\frac {Cov(X,Y)}{\sigma _{X}\sigma _{Y}}}\]</div>
</section>
<section id="correlation-de-spearman">
<h4>Corrélation de Spearman<a class="headerlink" href="#correlation-de-spearman" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Coefficient de corrélation de rang de Spearman.</div>
<div class="line">Montre la corrélation entre 2 variables avec un fonction non-linéaire.</div>
<div class="line">Coefficient de corrélation de Pearson appliqué aux rangs des données.</div>
</div>
<div class="math notranslate nohighlight">
\[r_{s}=\rho_{R(X),R(Y)}={\frac {Cov(R(X),R(Y))}{\sigma _{R(X)}\sigma _{R(Y)}}}\]</div>
</section>
<section id="notions-de-distances">
<h4>Notions de distances<a class="headerlink" href="#notions-de-distances" title="Lien permanent vers cette rubrique">#</a></h4>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/dm-euclidean.png"><img alt="_images/dm-euclidean.png" src="_images/dm-euclidean.png" style="width: 129.0px; height: 128.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Distance euclidienne</strong> : <span class="math notranslate nohighlight">\(\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}\)</span></span><a class="headerlink" href="#id1" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/dm-manhattan.png"><img alt="_images/dm-manhattan.png" src="_images/dm-manhattan.png" style="width: 129.0px; height: 128.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Distance de Manhattan</strong> : <span class="math notranslate nohighlight">\(|x_{1}-x_{2}|+|y_{1}-y_{2}|\)</span></span><a class="headerlink" href="#id2" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/dm-chebychev.png"><img alt="_images/dm-chebychev.png" src="_images/dm-chebychev.png" style="width: 129.0px; height: 128.0px;" /></a>
<figcaption>
<p><span class="caption-text"><strong>Distance de Chebychev</strong> : <span class="math notranslate nohighlight">\(\max(|x_{1}-x_{2}|,|y_{1}-y_{2}|)\)</span></span><a class="headerlink" href="#id3" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="feature-scaling">
<h4>Feature scaling<a class="headerlink" href="#feature-scaling" title="Lien permanent vers cette rubrique">#</a></h4>
<ul class="simple">
<li><p><strong>Normalisation</strong> : <span class="math notranslate nohighlight">\(x'=\frac{x-min(x)}{max(x)-min(x)}\;:\;x'\in[0,1]\)</span></p></li>
<li><p><strong>Normalisation à la moyenne</strong> : <span class="math notranslate nohighlight">\(x'=\frac{x-avg(x)}{max(x)-min(x)}\;:\;0=mean\)</span></p></li>
<li><p><strong>Standardisation</strong> : <span class="math notranslate nohighlight">\(x'=\frac{x-\bar{x}}{\sigma}\;:\;0\rightarrow mean\;:\;-1/+1\rightarrow std\)</span></p></li>
</ul>
</section>
</section>
<section id="regles-d-or">
<h3>Règles d’or<a class="headerlink" href="#regles-d-or" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Dans la vraie vie, les données ne suivent pas une distribution théorique ; les propriétés sont toujours corrélées ; les relations sont toujours non-linéaires.</div>
<div class="line"><strong>GIGO</strong> : Garbage In, Garbage out.</div>
</div>
</section>
</section>
<section id="clustering-beyond-k-means">
<h2>Clustering beyond k-means<a class="headerlink" href="#clustering-beyond-k-means" title="Lien permanent vers cette rubrique">#</a></h2>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Lien permanent vers cette rubrique">#</a></h3>
<section id="k-means">
<h4>K-means<a class="headerlink" href="#k-means" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">Pour un nombre de cluster <span class="math notranslate nohighlight">\(k\)</span>, trouver partition de données minimisant la distance inter-cluster, équivalent à :</div>
</div>
<ul class="simple">
<li><p>La distance carrée de points à leur centroïde.</p></li>
<li><p>La distance carrée entre les éléments d’un cluster.</p></li>
</ul>
</div>
<div class="math notranslate nohighlight">
\[argmin_{s}\sum_{i=1}^{k}\sum_{x\in S_{i}}||x-\mu_{i}||^{2} = argmin_{s}\sum_{i=1}^{k}|S_{i}|Var(S_{i})\]</div>
<div class="line-block">
<div class="line">Découvrir l’optimum gobal est NP-difficile.</div>
</div>
<ul class="simple">
<li><p>K-means naïf : Convergence vers un mauvais minimum local si un centroïde initial mauvais.</p></li>
<li><p>K-means++ : Améliore les résultats. A une tendance à chercher des clusters de mêmes tailles, et circulaires.</p></li>
</ul>
<div class="line-block">
<div class="line">Les données doivent être normalisées.</div>
</div>
</section>
</section>
<section id="melanges-gaussien-gaussian-mixtures">
<h3>Mélanges Gaussien (Gaussian mixtures)<a class="headerlink" href="#melanges-gaussien-gaussian-mixtures" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Chaque clusers peuvent être décrit avec une distribution normale centrée sur son centroïde, avec la probabilité d’observer un point descendant avec la distance au centroïde.</div>
</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">On définit un modèle génératif pour <span class="math notranslate nohighlight">\(k\)</span> clusters.</div>
</div>
<ul class="simple">
<li><p>Chaque cluster est définit par une distribution gaussiène définit par un centroïde et une matrice de variance ou covariance.</p></li>
<li><p>On doit trouver les paramètres <span class="math notranslate nohighlight">\(\Theta\)</span> (centres, variances) qui maximisent la probabilité d’observer les données <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>On cherche donc : <span class="math notranslate nohighlight">\(\Theta^{*}=argmax_{\Theta}P(X|\Theta)\)</span></p></li>
</ul>
</div>
<div class="line-block">
<div class="line">Permet de trouver des clusters de formes non-circulaires, de tailles différentes.</div>
<div class="line">On ajoute un paramètre de <em>force</em> <span class="math notranslate nohighlight">\(\pi\)</span> pour chaque cluster. <span class="math notranslate nohighlight">\(p(X) = \sum_{i=1}^{k}\pi_{i}G(X|\mu_{i},\sigma_{i})\)</span></div>
</div>
<div class="admonition-algorithme-em-expectation-maximization admonition">
<p class="admonition-title">Algorithme EM (Expectation Maximization)</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(Z\)</span> l’assignation sur les points pour leur plus grande probabilité d’appartenance à un cluster.</div>
</div>
<ol class="arabic simple">
<li><p>Initialiser <span class="math notranslate nohighlight">\(\Theta\)</span> aléatoirement.</p></li>
<li><p>Répéter jusqu’à convergence :</p>
<ol class="arabic simple">
<li><p><strong>E-step</strong> : Calculer <span class="math notranslate nohighlight">\(Z\)</span> pour <span class="math notranslate nohighlight">\(\Theta\)</span> courant.</p></li>
<li><p><strong>M-step</strong> : Actualiser <span class="math notranslate nohighlight">\(\Theta\)</span> avec <span class="math notranslate nohighlight">\(Z\)</span> courant.</p></li>
</ol>
</li>
</ol>
</div>
<div class="line-block">
<div class="line">Utiliser la <strong>Minimum Description Length (MDL)</strong> pour choisir le nombre de clusters.</div>
</div>
</section>
<section id="dbscan">
<h3>DBSCAN<a class="headerlink" href="#dbscan" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Définition de paramètres locaux :</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> : La distance maximale pour que 2 points ne soient pas considérés comme différents.</p></li>
<li><p><strong>minPts</strong> : Le nombre minimum de points atteignables.</p></li>
<li><p>Pas de défintion de nombre de clusters.</p></li>
<li><p><strong>Core point</strong> : Un point avec au moins <span class="math notranslate nohighlight">\(minPts\)</span> points dans un rayon de <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p></li>
</ul>
<div class="admonition-algorithme admonition">
<p class="admonition-title">Algorithme</p>
<ol class="arabic simple">
<li><p>Créer un graphes avec les points comme noeuds et les arcs entre les points à une distance inférieure à <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p></li>
<li><p>Détecter les composantes connexes du graphe.</p></li>
<li><p>Pour les noeuds non-core. Si il y a aucun <em>core point</em> atteignable, on l’oublie tel du bruit. Sinon, on l’ajoute à la composante connexe du <em>core point</em> atteignable.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line"><strong>Forces</strong> : Pas besoin de définir un nombre de clusters ; peut découvrir des formes arbitraires ; notion de bruit.</div>
<div class="line"><strong>Faiblesses</strong> : Définir <span class="math notranslate nohighlight">\(\epsilon\)</span> est compliqué ; risque d’étirement des clusters.</div>
</div>
</section>
<section id="evaluation-de-clustering">
<h3>Evaluation de clustering<a class="headerlink" href="#evaluation-de-clustering" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">2 types d’évaluation :</div>
</div>
<ul class="simple">
<li><p><strong>External evaluation (extrinsic)</strong> : Il y a un <em>ground truth</em>, qui peut être une approximation de ce que l’on veut.</p></li>
<li><p><strong>Internal evaluation (intrinsic)</strong> : Il n’y a pas de <em>ground truth</em>. On évalue les propriétés intrinsèques des clusters.</p></li>
</ul>
<section id="score-silhouette">
<h4>Score silhouette<a class="headerlink" href="#score-silhouette" title="Lien permanent vers cette rubrique">#</a></h4>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a(i)\)</span> : La distance moyenne entre <span class="math notranslate nohighlight">\(i\)</span> et les autres points du cluster.</p></li>
<li><p><span class="math notranslate nohighlight">\(b(i)\)</span> : Le minimum des distances moyennes entre <span class="math notranslate nohighlight">\(i\)</span> et les points du cluster le plus proche.</p></li>
<li><p><span class="math notranslate nohighlight">\(s(i)\)</span> =</p>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(a(i) &lt; b(i)\)</span> : <span class="math notranslate nohighlight">\(1 - \frac{a(i)}{b(i)}\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(a(i) &gt; b(i)\)</span> : <span class="math notranslate nohighlight">\(\frac{b(i)}{a(i)} - 1\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(a(i) = b(i)\)</span> : <span class="math notranslate nohighlight">\(0\)</span></p></li>
</ul>
</li>
</ol>
<div class="line-block">
<div class="line">Le coefficient silhouette est la moyenne des <span class="math notranslate nohighlight">\(s(i)\)</span> pour tous les points.</div>
</div>
</section>
<section id="davies-bouldin-index-dbi">
<h4>Davies-Bouldin Index (DBI)<a class="headerlink" href="#davies-bouldin-index-dbi" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">La moyenne des ratios de similarité de chaque clusters avec le cluster le plus proche.</div>
<div class="line">Une valeur faible indique des clusters bien séparés.</div>
</div>
</section>
<section id="dunn-index">
<h4>Dunn Index<a class="headerlink" href="#dunn-index" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="math notranslate nohighlight">
\[DI_{m} = \frac{min_{1\leq i &lt; j \leq m} \delta(C_{i}, C_{j})}{max_{1\leq k \leq m} \Delta_{k}}\]</div>
<div class="line-block">
<div class="line">Avec :</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta(C_{i}, C_{j})\)</span> : La distance entre les clusters <span class="math notranslate nohighlight">\(C_{i}\)</span> et <span class="math notranslate nohighlight">\(C_{j}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta_{k}\)</span> : Le diamètre du cluster <span class="math notranslate nohighlight">\(C_{k}\)</span>.</p></li>
</ul>
</section>
</section>
</section>
<section id="networks-i-centralities">
<h2>Networks I - Centralities<a class="headerlink" href="#networks-i-centralities" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="admonition-notation-d-un-graphe admonition">
<p class="admonition-title">Notation d’un graphe</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(G=(V,E)\)</span></div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V\)</span> est l’ensemble des noeuds (ou sommets) du graphe.</p></li>
<li><p><span class="math notranslate nohighlight">\(E\)</span> est l’ensemble des arêtes (ou liens) du graphe.</p></li>
<li><p><span class="math notranslate nohighlight">\(u \in V\)</span> est un noeud du graphe.</p></li>
<li><p><span class="math notranslate nohighlight">\((u,v) \in E\)</span> est une arête du graphe.</p></li>
</ul>
</div>
<div class="admonition-densite admonition">
<p class="admonition-title">Densité</p>
<ul class="simple">
<li><p><strong>Degré moyen</strong> : <span class="math notranslate nohighlight">\(&lt;k&gt; = \frac{2|E|}{|V|}\)</span></p></li>
<li><p><strong>Densité</strong> : <span class="math notranslate nohighlight">\(d = \frac{2|E|}{|V|(|V|-1)}\)</span></p></li>
<li><p><strong>Coefficient de clustering</strong> : Les triangles sont importants dans les vraies réseaux. <span class="math notranslate nohighlight">\(C_{u} = d(H(N_{u}))\)</span></p></li>
<li><p><strong>Moyenne du coefficient de clustering</strong> : <span class="math notranslate nohighlight">\(&lt;C&gt; = \frac{1}{|V|}\sum_{u \in V} C_{u}\)</span></p></li>
<li><p><strong>Coefficient de clustering global</strong> : <span class="math notranslate nohighlight">\(C^{G} = \frac{3\Delta}{\Delta^{max}}\)</span></p></li>
</ul>
</div>
<div class="admonition-paths-walks-distance admonition">
<p class="admonition-title">Paths, Walks, Distance</p>
<ul class="simple">
<li><p><strong>Walk</strong> : Séquence de noeuds adjacents.</p></li>
<li><p><strong>Path</strong> : <em>Walk</em> sans noeuds répétés.</p></li>
<li><p><strong>Path length</strong> : Nombre d’arêtes dans un <em>Path</em>.</p></li>
<li><p><strong>Weighted path length</strong> : Somme des poids des arêtes dans un <em>Path</em>.</p></li>
<li><p><strong>Shortest path</strong> : <em>Path</em> de longueur minimale.</p></li>
<li><p><strong>Weighted shortest path</strong> : <em>Path</em> de poids minimal.</p></li>
<li><p><strong>Distance</strong> : Longueur du <em>Shortest path</em>.</p></li>
<li><p><strong>Diamètre</strong> : Distance maximale entre deux noeuds du graphe. <span class="math notranslate nohighlight">\(&lt;l&gt; = \frac{1}{|V|(|V|-1)}\sum_{i \neq j} d_{ij}\)</span></p></li>
</ul>
</div>
<section id="centralitees">
<h3>Centralitées<a class="headerlink" href="#centralitees" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">On peut mesurer l’importance d’un noeud avec la <strong>centralitée</strong>.</div>
</div>
<ul class="simple">
<li><p><strong>Farness</strong> : Distance moyenne vers tous les noeuds du graphe. <span class="math notranslate nohighlight">\(farness(u) = \frac{1}{|V|-1} \sum_{v \in V, v \neq u} l_{u,v}\)</span></p></li>
<li><p><strong>Closeness</strong> : Inverse de la farness. <span class="math notranslate nohighlight">\(closeness(u) = \frac{|V|-1}{\sum_{v \in V, v \neq u} l_{u,v}}\)</span></p></li>
<li><p><strong>Centralité harmonique</strong> : <span class="math notranslate nohighlight">\(harmonic(u) = \frac{1}{|V|-1} \sum_{v \in V, v \neq u} \frac{1}{l_{u,v}}\)</span></p></li>
<li><p><strong>Centralité de proximité</strong> : Mesure comment un noeud joue un rôle de pont. <span class="math notranslate nohighlight">\(C_{B}(v) = \sum_{s \neq v \neq t \in V} \frac{\sigma_{st}(v)}{\sigma_{st}}\)</span> avec <span class="math notranslate nohighlight">\(\sigma_{st}\)</span> le nombre de <em>Shortest path</em> de <span class="math notranslate nohighlight">\(s\)</span> à <span class="math notranslate nohighlight">\(t\)</span> et <span class="math notranslate nohighlight">\(\sigma_{st}(v)\)</span> le nombre de <em>Shortest path</em> de <span class="math notranslate nohighlight">\(s\)</span> à <span class="math notranslate nohighlight">\(t\)</span> passant par <span class="math notranslate nohighlight">\(v\)</span>.</p></li>
</ul>
</section>
<section id="definitions-recursives">
<h3>Définitions récursives<a class="headerlink" href="#definitions-recursives" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Les <strong>noeuds importants</strong> sont ceux qui sont connectées à des <strong>noeuds importants</strong>.</div>
</div>
<div class="math notranslate nohighlight">
\[C_{u}^{t+1} = \frac{1}{\lambda} \sum_{v \in N_{u}} C_{v}^{t}\]</div>
<div class="line-block">
<div class="line">Avec : <span class="math notranslate nohighlight">\(\lambda\)</span> un facteur de normalisation.</div>
</div>
<section id="pagerank">
<h4>PageRank<a class="headerlink" href="#pagerank" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">2 améliorations principales :</div>
</div>
<ul class="simple">
<li><p>Sur les graphes dirigées, problème de noeuds sources : on ajoute un gain central constant sur tous les noeuds.</p></li>
<li><p>Les noeuds avec une grande centralitée la donne à leurs voisins.</p></li>
</ul>
<div class="line-block">
<div class="line">Idée de base : PageRank peut être interprété comme un <em>random walk</em> avec un restart.</div>
</div>
</section>
</section>
</section>
<section id="networks-ii-detection-de-communautes">
<h2>Networks II - Détection de communautés<a class="headerlink" href="#networks-ii-detection-de-communautes" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="line-block">
<div class="line">La détection de communautés est comme le problème de clustering, mais sur des graphes.</div>
</div>
<section id="quelques-algos">
<h3>Quelques algos<a class="headerlink" href="#quelques-algos" title="Lien permanent vers cette rubrique">#</a></h3>
<section id="girvan-newman">
<h4>Girvan &amp; Newman<a class="headerlink" href="#girvan-newman" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="admonition-algorithme-de-girvan-newman admonition">
<p class="admonition-title">Algorithme de Girvan &amp; Newman</p>
<ol class="arabic simple">
<li><p>Calculer la <em>betweenness</em> de chaque arêtes.</p></li>
<li><p>Enlever l’arête avec la plus grande <em>betweenness</em>.</p></li>
<li><p>Répéter jusqu’à ce qu’il n’y ait plus d’arêtes.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line">Introduction de la <strong>modularité</strong>. On la calcule sur une partition du graphe.</div>
<div class="line">Elle compare le nombre d’arêtes dans une communauté avec le nombre d’arêtes attendues dans une communauté aléatoire.</div>
</div>
<div class="math notranslate nohighlight">
\[Q = \frac{1}{(2m)} \sum_{vw}[A_{vw} - \frac{k_{v}k_{w}}{2m}]\delta(c_{v},c_{w})\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta(c_{v},c_{w})\)</span> : 1 si <span class="math notranslate nohighlight">\(v\)</span> et <span class="math notranslate nohighlight">\(w\)</span> sont dans la même communauté, 0 sinon.</p></li>
<li><p><span class="math notranslate nohighlight">\(A_{vw}\)</span> : 1 si <span class="math notranslate nohighlight">\(v\)</span> et <span class="math notranslate nohighlight">\(w\)</span> sont connectés, 0 sinon.</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{k_{v}k_{w}}{2m}\)</span> : Probabilité que <span class="math notranslate nohighlight">\(v\)</span> et <span class="math notranslate nohighlight">\(w\)</span> soient connectés dans un graphe aléatoire.</p></li>
</ul>
<div class="line-block">
<div class="line">La modularité compare le réseau avec un <strong>null network</strong>.</div>
<div class="line">Le but de la méthode de Girvan et Newman et d’optimiser la modularité.</div>
</div>
</section>
<section id="louvain">
<h4>Louvain<a class="headerlink" href="#louvain" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="admonition-algorithme-de-louvain admonition">
<p class="admonition-title">Algorithme de Louvain</p>
<ol class="arabic simple">
<li><p>Initialiser chaque noeud dans sa propre communauté.</p></li>
<li><p>Répéter jusqu’à convergence : pour chaque noeuds, pour chaque voisins, si ajouter le noeud à la communauté du voisin augmente la modularité, on le fait.</p></li>
<li><p>Création d’un ensemble induit : chaque communauté devient un noeud, et les arêtes sont la somme des poids entre les communautés.</p></li>
</ol>
</div>
</section>
<section id="infomap">
<h4>Infomap<a class="headerlink" href="#infomap" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Trouver la partition qui minimise la description de n’importe quel <em>random walk</em> sur le graphe. On veut compresser la description des random walks.</div>
<div class="line">Infomap définit une fonction de qualité à la place de la modularité. Tout algorithme peut être utilisé pour optimiser cette fonction.</div>
<div class="line">Infomap peut reconnaître des random walks, pas des communautés.</div>
</div>
</section>
<section id="stochastic-block-model-sbm">
<h4>Stochastic Block Model (SBM)<a class="headerlink" href="#stochastic-block-model-sbm" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line"><strong>SBM</strong> est basé sur les statistiques.</div>
<div class="line">Chaque noeud correspond à une seule communauté. Pour chaque paire de communauté, on associe une densité qui est la probabilité qu’une arrête existe.</div>
<div class="line">On précise le nombre de clusters.</div>
</div>
</section>
</section>
<section id="evaluation-de-structure-de-communautes">
<h3>Evaluation de structure de communautés<a class="headerlink" href="#evaluation-de-structure-de-communautes" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Comme pour le clustering :</div>
</div>
<ul class="simple">
<li><p><strong>Intrinsèque / interne</strong> : Fonction de qualité de partition, fonction de qualité des communautés.</p></li>
<li><p><strong>Comparaison des communautés observés et attendues</strong> : Vrai réseau avec <em>ground truth</em>.</p></li>
</ul>
</section>
<section id="prediction-de-liens">
<h3>Prédiction de liens<a class="headerlink" href="#prediction-de-liens" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Quels sont les liens manquants ou futurs ?</div>
</div>
<ul class="simple">
<li><p>Basé sur les <strong>graphes</strong> :</p>
<ul>
<li><p><strong>Local</strong> : Grand clustering</p></li>
<li><p><strong>Global</strong> : 2 hubs non reliés ont plus de chance d’avoir des liens sur des petits noeuds pas liés.</p></li>
<li><p><strong>Meso-scale</strong> : Des probabilités de liens différentes pour des noeuds de communautés différentes.</p></li>
</ul>
</li>
<li><p>Basé sur les <strong>informations des noeuds</strong> :</p>
<ul>
<li><p>Sur les <strong>features</strong> même.</p></li>
<li><p>Combiner avec du <strong>ML</strong>.</p></li>
</ul>
</li>
</ul>
<section id="premiere-approche-non-supervise-heuristique">
<h4>Première approche (Non-supervisé) - Heuristique<a class="headerlink" href="#premiere-approche-non-supervise-heuristique" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Le principe est de donner un score basé sur la topologie du graphe <span class="math notranslate nohighlight">\(f(v_{i},v_{j})\)</span> exprimant la probabilité que <span class="math notranslate nohighlight">\(v_{i}\)</span> et <span class="math notranslate nohighlight">\(v_{j}\)</span> soient liés.</div>
<div class="line"><span class="math notranslate nohighlight">\(\Gamma(x)\)</span> : les voisins de <span class="math notranslate nohighlight">\(x\)</span>.</div>
</div>
<div class="admonition-common-neighbour-cn admonition">
<p class="admonition-title">Common Neighbour (CN)</p>
<div class="line-block">
<div class="line"><em>Les amis des mes amis sont mes amis.</em></div>
<div class="line"><span class="math notranslate nohighlight">\(CN(x,y) = |\Gamma(x) \cap \Gamma(y)|\)</span></div>
</div>
</div>
<div class="admonition-coefficicent-de-jaccard admonition">
<p class="admonition-title">Coefficicent de Jaccard</p>
<div class="line-block">
<div class="line">Intuition :</div>
</div>
<ul class="simple">
<li><p><strong>Haute probabilité</strong> : 2 personnes qui connaissent seulement les mêmes 3 personnes.</p></li>
<li><p><strong>Basse probabilité</strong> : 2 personnes qui connaissent 3 personnes sur 1000.</p></li>
</ul>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(JC(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x) \cup \Gamma(y)|}\)</span></div>
</div>
</div>
<div class="admonition-hub-promoted admonition">
<p class="admonition-title">Hub promoted</p>
<div class="line-block">
<div class="line">Intuition : Normaliser par le nombre de voisins.</div>
<div class="line"><span class="math notranslate nohighlight">\(HP(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{min(|\Gamma(x)|,|\Gamma(y)|)}\)</span></div>
</div>
</div>
<div class="admonition-adamic-adar admonition">
<p class="admonition-title">Adamic adar</p>
<div class="line-block">
<div class="line">Intuition : Un noeud connecté seulement à une personne représente plus qu’un noeud connecté à 1000 personnes.</div>
<div class="line"><span class="math notranslate nohighlight">\(AA(x,y) = \sum_{z \in \Gamma(x) \cap \Gamma(y)} \frac{1}{log(|\Gamma(z)|)}\)</span></div>
</div>
</div>
<div class="admonition-resource-allocation admonition">
<p class="admonition-title">Resource allocation</p>
<div class="line-block">
<div class="line">Pénalise plus qu’Adamic Adar.</div>
<div class="line"><span class="math notranslate nohighlight">\(RA(x,y) = \sum_{z \in \Gamma(x) \cap \Gamma(y)} \frac{1}{|\Gamma(z)|}\)</span></div>
</div>
</div>
<div class="admonition-preferential-attachment admonition">
<p class="admonition-title">Preferential attachment</p>
<div class="line-block">
<div class="line">Intuition : Les noeuds avec beaucoup de voisins ont plus de chance d’avoir des liens avec des nouveaux noeuds.</div>
<div class="line"><span class="math notranslate nohighlight">\(PA(x,y) = |\Gamma(x)| \cdot |\Gamma(y)|\)</span></div>
</div>
</div>
<div class="admonition-d-autres-scores admonition">
<p class="admonition-title">D’autres scores</p>
<div class="line-block">
<div class="line"><strong>Sorenson Index</strong> : <span class="math notranslate nohighlight">\(SI(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x)| + |\Gamma(y)|}\)</span></div>
<div class="line"><strong>Salton Cosine Similarity</strong> : <span class="math notranslate nohighlight">\(SC(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{\sqrt{|\Gamma(x)| \cdot |\Gamma(y)|}}\)</span></div>
<div class="line"><strong>Hub Depressed</strong> : <span class="math notranslate nohighlight">\(HD(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{max(|\Gamma(x)|,|\Gamma(y)|)}\)</span></div>
<div class="line"><strong>Leicht-Holme-Nerman</strong> : <span class="math notranslate nohighlight">\(LHN(x,y) = \frac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x)| \cdot |\Gamma(y)|}\)</span></div>
</div>
</div>
<div class="admonition-structure-de-communautes admonition">
<p class="admonition-title">Structure de communautéS</p>
<ul class="simple">
<li><p>On calcule la structure de communautés sur le graphe.</p></li>
<li><p>Si les noeuds sont dans la même communauté, on assigne un gros score, sinon un petit score.</p></li>
</ul>
</div>
</section>
<section id="seconde-approche-supervise-score-de-similarite">
<h4>Seconde approche (Supervisé) - Score de similarité<a class="headerlink" href="#seconde-approche-supervise-score-de-similarite" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">On utilise les algorithmes de ML pour apprendre à combiner les heuristique.</div>
<div class="line">On utilise les heuristiques comme features.</div>
</div>
</section>
</section>
<section id="classification-de-noeuds">
<h3>Classification de noeuds<a class="headerlink" href="#classification-de-noeuds" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">On veut prédire la classe d’un noeud :</div>
</div>
<ul class="simple">
<li><p>Valeurs manquantes.</p></li>
<li><p>Apprendre des informations sur des personnes sur des réseaux sociaux.</p></li>
<li><p>Type d’article sur Wikipedia.</p></li>
</ul>
<section id="methodes-recentes">
<h4>Méthodes récentes<a class="headerlink" href="#methodes-recentes" title="Lien permanent vers cette rubrique">#</a></h4>
<ul class="simple">
<li><p><strong>Deep Neural Network (DNN)</strong> : Utiliser comme <em>state of the art</em> pour les tâches supervisées.</p></li>
<li><p><strong>Graph Convolutional Neural Network (GCN)</strong> : Prédiction de liens ; classification de noeuds ; classification de graphes ; etc.</p></li>
<li><p><strong>Variational Graph Autoencoder (VGA)</strong> : Prédiction de liens ; graph embedding ; etc.</p></li>
<li><p><strong>Graph Attention Networks (GAT)</strong> : Méchanisme d’attention pour les graphes.</p></li>
<li><p><strong>Diffusion, Convutionnal, Recurrent NN (DCRNN)</strong> : Données dynamiques.</p></li>
</ul>
<p>Pour plus d’informations : <a class="reference internal" href="biml.html#gnn"><span class="std std-ref">Graph Neural Network (GNN)</span></a></p>
</section>
</section>
</section>
<section id="recommendation-factorisation-matricielle">
<h2>Recommendation / Factorisation matricielle<a class="headerlink" href="#recommendation-factorisation-matricielle" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="line-block">
<div class="line">Le <strong>facteur latent</strong> est un problème populaire de DM. Avec 2 types de données, comment reconstruire une information. <em>(tâche non supervisée)</em></div>
<div class="line">Les <strong>recommendations basées sur le contenu</strong> décrivent les objets avec des features. On recommande des objets qui ont des features similaires. <em>Souvent, pas très efficace.</em></div>
<div class="line">Le <strong>filtrage collaboratif</strong> utilise les utilisateurs comme features.</div>
</div>
<section id="definition-du-modele">
<h3>Définition du modèle<a class="headerlink" href="#definition-du-modele" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">On modèlise les données avec une matrice <span class="math notranslate nohighlight">\(U \times I\)</span>. Avec <span class="math notranslate nohighlight">\(U\)</span> les utilisateurs et <span class="math notranslate nohighlight">\(I\)</span> les items.</div>
<div class="line"><span class="math notranslate nohighlight">\(X(u, i)\)</span> reprèsente les intéractions entre les utilisateurs et les items.</div>
</div>
<div class="admonition-extraire-une-baseline admonition">
<p class="admonition-title">Extraire une baseline</p>
<div class="line-block">
<div class="line">On estine une score de baseline <span class="math notranslate nohighlight">\((u,i)\)</span> basé sur 2 constantes : <span class="math notranslate nohighlight">\(b_{u}\)</span>, <span class="math notranslate nohighlight">\(b_{i}\)</span>.</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b_{u}\)</span> : Capture la tendance de <span class="math notranslate nohighlight">\(u\)</span> a donner des grands ou petits scores.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_{i}\)</span> : Capture la tendance de <span class="math notranslate nohighlight">\(i\)</span> a recevoir des petits ou grands scores.</p></li>
<li><p>On minimise : <span class="math notranslate nohighlight">\(\sum_{r_{ui} \in R_{train}} (r_{ui}-(\mu + b_{u} + b_{i}))^{2}+\lambda (b_{u}^{2} + b_{i}^{2})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> : Note moyenne pour un utilisateur et un item aléatoire.</p></li>
</ul>
</div>
</section>
<section id="user-based-knn">
<h3>User-based KNN<a class="headerlink" href="#user-based-knn" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-k-nearest-neighbors admonition">
<p class="admonition-title">K-Nearest-Neighbors</p>
<ol class="arabic simple">
<li><p>Trouver <span class="math notranslate nohighlight">\(k\)</span> voisins les plus similaires d’un item.</p></li>
<li><p>Prédire la note de l’item comme la moyenne des notes des <span class="math notranslate nohighlight">\(k\)</span> voisins.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line">Appliqué aux utilisateurs :</div>
</div>
<ol class="arabic simple">
<li><p>Trouver les <span class="math notranslate nohighlight">\(k\)</span> voisins les plus similaires.</p></li>
<li><p>Chaque voisins <em>vote</em> pour les items qu’ils aiment.</p></li>
</ol>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/dm-knn.png"><img alt="_images/dm-knn.png" src="_images/dm-knn.png" style="width: 477.0px; height: 360.0px;" /></a>
<figcaption>
<p><span class="caption-text">User-based KNN</span><a class="headerlink" href="#id4" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<div class="line-block">
<div class="line">Calculer la similarité entre des utilisateurs :</div>
</div>
<ul class="simple">
<li><p><strong>Jaccard similarity</strong> <em>(binaire)</em> : <span class="math notranslate nohighlight">\(\frac{|likes(u \&amp; v)|}{(union like)}\)</span></p></li>
<li><p><strong>Mean Squared Difference (MSD)</strong> <em>(notes)</em></p></li>
<li><p><strong>Cosine similarity</strong> <em>(notes)</em> : <span class="math notranslate nohighlight">\(cos(\Theta) = \frac{A \cdot B}{||A|| \cdot ||B||} = \frac{\sum_{i=1}^{n} A_{i}B_{i}}{\sqrt{\sum_{i=1}^{n} A_{i}^{2}} \sqrt{\sum_{i=1}^{n} B_{i}^{2}}}\)</span></p></li>
<li><p><strong>Cosine similarity</strong> <em>(binaire)</em> : <span class="math notranslate nohighlight">\(\frac{|likes(u \&amp; v)|}{\sqrt{|likes(u)|} \sqrt{|likes(v)|}}\)</span></p></li>
</ul>
</section>
<section id="item-based-collaborative-filtering">
<h3>Item-based Collaborative Filtering<a class="headerlink" href="#item-based-collaborative-filtering" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">On veut évaluer l’intérêt de <span class="math notranslate nohighlight">\((u,i)\)</span> :</div>
</div>
<ol class="arabic simple">
<li><p>Pour chaque item <span class="math notranslate nohighlight">\(x\)</span> aimé par <span class="math notranslate nohighlight">\(u\)</span>, on calcule la similarité entre <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((u,i)\)</span> est la moyenne des similarité de <span class="math notranslate nohighlight">\((x,i)\)</span> pour chaque item <span class="math notranslate nohighlight">\(x\)</span> aimé par <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
</ol>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="_images/dm-ibcf.png"><img alt="_images/dm-ibcf.png" src="_images/dm-ibcf.png" style="width: 504.0px; height: 435.0px;" /></a>
<figcaption>
<p><span class="caption-text">Item-based Collaborative Filtering</span><a class="headerlink" href="#id5" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="matrix-factorization-collaborative-filtering">
<h3>Matrix Factorization Collaborative Filtering<a class="headerlink" href="#matrix-factorization-collaborative-filtering" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>On commence avec une matrice <span class="math notranslate nohighlight">\(A\)</span> <em>(type user/interaction)</em>.</p></li>
<li><p>On cherche 2 matrices <span class="math notranslate nohighlight">\(U\)</span> et <span class="math notranslate nohighlight">\(V\)</span>, afin de minimiser une fonction de coût <span class="math notranslate nohighlight">\(L(A, UV)\)</span>.</p></li>
</ul>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="_images/dm-mfcf.png"><img alt="_images/dm-mfcf.png" src="_images/dm-mfcf.png" style="width: 696.0px; height: 217.0px;" /></a>
<figcaption>
<p><span class="caption-text">Matrix Factorization Collaborative Filtering</span><a class="headerlink" href="#id6" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="_images/dm-objFunc.png"><img alt="_images/dm-objFunc.png" src="_images/dm-objFunc.png" style="width: 616.0px; height: 205.0px;" /></a>
<figcaption>
<p><span class="caption-text">Objective functions</span><a class="headerlink" href="#id7" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<div class="admonition-optmisation admonition">
<p class="admonition-title">Optmisation</p>
<div class="line-block">
<div class="line"><strong>Weighted Alternating Least Square (WALS)</strong></div>
</div>
<ol class="arabic simple">
<li><p>Initialiser à des valeurs aléatoires.</p></li>
<li><p>Répéter jusqu’à convergence :</p>
<ol class="arabic simple">
<li><p>Fixer <span class="math notranslate nohighlight">\(U\)</span> et résoudre <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p>Fixer <span class="math notranslate nohighlight">\(V\)</span> et résoudre <span class="math notranslate nohighlight">\(U\)</span>.</p></li>
</ol>
</li>
</ol>
</div>
<ul class="simple">
<li><p><strong>MF + Regularization</strong> : <span class="math notranslate nohighlight">\(\sum_{r_{ui} \in obs} (r_{ui}-\hat{r}_{ui})^{2} + \lambda (||q_{i}||^{2} + ||p_{u}||^{2})\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(q_{i}\)</span> et <span class="math notranslate nohighlight">\(p_{u}\)</span> : espaces latents.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> : controle la force de la régularisation.</p></li>
</ul>
</li>
<li><p><strong>MF + Baseline</strong> : <span class="math notranslate nohighlight">\(\sum_{r_{ui} \in obs} (r_{ui}-\hat{r}_{ui})^{2} + \lambda (b_{i}^{2} + b_{u}^{2} + ||q_{i}||^{2} + ||p_{u}||^{2})\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(b_{i}\)</span> et <span class="math notranslate nohighlight">\(b_{u}\)</span> : scores espérés pour les items et les utilisateurs.</p></li>
</ul>
</li>
</ul>
</section>
<section id="evaluation-des-systemes-de-recommandations">
<h3>Evaluation des systèmes de recommandations<a class="headerlink" href="#evaluation-des-systemes-de-recommandations" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Cacher des utilisateurs :</p>
<ol class="arabic simple">
<li><p>On entraine avec toutes les données sur une partition des utilisateurs.</p></li>
<li><p>On valide sur les autres utilisateurs, considérés comme nouveaux.</p></li>
</ol>
</li>
<li><p>Cacher des paires <span class="math notranslate nohighlight">\((u,i)\)</span> :</p>
<ol class="arabic simple">
<li><p>On cache un nombre de paires <span class="math notranslate nohighlight">\((u,i)\)</span> aléatoirement.</p></li>
<li><p>On évalue sur ces paires cachées.</p></li>
</ol>
</li>
</ul>
</section>
<section id="variante-de-mf-non-negative-matrix-factorization-nmf">
<h3>Variante de MF - Non-negative Matrix Factorization (NMF)<a class="headerlink" href="#variante-de-mf-non-negative-matrix-factorization-nmf" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">On veut des facteurs positifs, puisque certaines variables pouvaient s’annuler dans un MF classique.</div>
</div>
</section>
<section id="co-clustering">
<h3>Co-Clustering<a class="headerlink" href="#co-clustering" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">L’objectif est de trouver des sous-matrices denses au sein d’une matrice.</div>
</div>
<div class="math notranslate nohighlight">
\[Q = \sum_{i}^{n} \sum_{j}^{d} A_{ij} - \frac{k_{i}k_{j}}{|A|}\delta_{ij}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> : matrice à co-clusterisé, dimension <span class="math notranslate nohighlight">\(n \times d\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(k_{i}\)</span> : le degré pondéré de <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = 1\)</span> : <span class="math notranslate nohighlight">\(i,j\)</span> sont du même co-cluster.</p></li>
<li><p><span class="math notranslate nohighlight">\(|A|\)</span> : Somme de toutes les valeurs dans la matrice.</p></li>
</ul>
</section>
</section>
<section id="reduction-de-dimension-low-dimensionality-embedding">
<h2>Reduction de dimension <em>Low dimensionality embedding</em><a class="headerlink" href="#reduction-de-dimension-low-dimensionality-embedding" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="line-block">
<div class="line">Avoir des centaines/milliers d’attributs est un problème pour l’analyse de données. On veut donc réduire le nombre d’attributs, tout en gardant les informations importantes.</div>
<div class="line">La réduction de dimensions peut créer une variable seule qui capture ce qui est commun.</div>
</div>
<section id="pca">
<h3>PCA<a class="headerlink" href="#pca" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">La <strong>Principal Component Analysis (PCA)</strong> définit de nouvelles dimensions qui sont des combinaisons linéaires des dimensions originales.</div>
<div class="line">L’objectif est de concentrer la <em>variance</em> sur certaines dimensions.</div>
</div>
</div>
<div class="admonition-algorithme admonition">
<p class="admonition-title">Algorithme</p>
<ol class="arabic simple">
<li><p>Trouver un <em>axe</em>, un vecteur qui minimise la variance. <em>(la distance carré de tout les points à cet axe)</em></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(d\)</span> in <span class="math notranslate nohighlight">\((initial_d - 1)\)</span> : Trouver un autre axe orthogonal à tout les précédents qui minimise la variance.</p></li>
<li><p>Garder la première k-dimension.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line">En pratique on cherhce les vecteurs propres de la matrice de covariance de la matrice de données centrées. Si on veut <span class="math notranslate nohighlight">\(k\)</span> nouvelles dimensions, on garde les <span class="math notranslate nohighlight">\(k\)</span> vecteurs propres avec les plus grandes valeurs propres.</div>
</div>
</section>
<section id="manifolds">
<h3>Manifolds<a class="headerlink" href="#manifolds" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-principe-general admonition">
<p class="admonition-title">Principe général</p>
<ol class="arabic simple">
<li><p>Définir une notion de distance entre les éléments dans l’espace original.</p></li>
<li><p>Définir une notion de distance entre les éléments dans l’espace réduit.</p></li>
<li><p>Minimiser la différence entre les distances.</p></li>
</ol>
</div>
<section id="mds">
<h4>MDS<a class="headerlink" href="#mds" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">La <strong>Multi-Dimensional Scaling (MDS)</strong> minimise simplement la distance entre l’espace original et l’espace réduit.</div>
</div>
<div class="admonition-algorithme admonition">
<p class="admonition-title">Algorithme</p>
<ol class="arabic simple">
<li><p>Calculer toutes les distances entre les objets, ce qui donne une matrice de similarité.</p></li>
<li><p>Calculer le PCA de la matrice de similarité.</p></li>
</ol>
</div>
</section>
<section id="isomap">
<h4>Isomap<a class="headerlink" href="#isomap" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line">Une variation de MDS.</div>
</div>
<ol class="arabic simple">
<li><p>On définit un graphe tel que les noeuds sont connectés si leur distance est inférieure à un threshold.</p></li>
<li><p>Calcule de la matrice de similarité, tel que la distance soit la distance du plus court chemin dans le graphe pondéré.</p></li>
<li><p>Appliquer MDS.</p></li>
</ol>
</section>
</section>
<section id="t-sne">
<h3>T-SNE<a class="headerlink" href="#t-sne" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">La <strong>t-distributed Stochastic Neighbor Embedding (t-SNE)</strong> est une méthode non-linéaire de réduction de dimensions. <em>(actuellement la plus populaire)</em></div>
</div>
<div class="line-block">
<div class="line">Distance dans l’espace original <span class="math notranslate nohighlight">\(P\)</span> :</div>
</div>
<ul class="simple">
<li><p>Pour calculer à quel point <span class="math notranslate nohighlight">\(j\)</span> est loin de <span class="math notranslate nohighlight">\(i\)</span>, on considère une distribution normal centrée en <span class="math notranslate nohighlight">\(j\)</span> avec une variance de <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p>Mathématiquement : la distance est donnée avec <span class="math notranslate nohighlight">\(s_{j|i}^{P} = e^{-\frac{||x_{i}-x_{j}||^{2}}{2\sigma^{2}}}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{j|i} = \frac{s_{j|i}^{P}}{\sum_{k \neq i} s_{j|k}^{P}}\)</span> : La probabilité que <span class="math notranslate nohighlight">\(j\)</span> soit un voisin de <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(\sigma\)</span> faible préserve surtout les distances locales. <span class="math notranslate nohighlight">\(\sigma\)</span> donne plus d’importance aux longues globales.</div>
</div>
</section>
<section id="low-dimensionality-embedding">
<h3>Low dimensionality embedding<a class="headerlink" href="#low-dimensionality-embedding" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Actuellement l’usage de la réduction de dimensions sert à encoder des objets complexes en vecteurs.</div>
</div>
<div class="admonition-exemples admonition">
<p class="admonition-title">Exemples</p>
<ul class="simple">
<li><p><strong>Word2Vec</strong> : Réduction de dimensions pour les mots.</p></li>
<li><p><strong>Doc2Vec</strong> : Réduction de dimensions pour les documents.</p></li>
<li><p><strong>Node2Vec</strong> : Réduction de dimensions pour les noeuds de graphes.</p></li>
</ul>
</div>
</section>
<section id="word-embedding">
<h3>Word Embedding<a class="headerlink" href="#word-embedding" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Vu dans le cours de BIML : <a class="reference internal" href="biml.html#biml-we"><span class="std std-ref">Word embedding</span></a>.</div>
</div>
</section>
<section id="graph-embedding">
<h3>Graph Embedding<a class="headerlink" href="#graph-embedding" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-skipgram-generique admonition">
<p class="admonition-title"><em>Skipgram</em> générique</p>
<ul class="simple">
<li><p>Prend en entrée :</p>
<ul>
<li><p>L’élément à englober.</p></li>
<li><p>Une liste de <em>contexte</em>.</p></li>
</ul>
</li>
<li><p>Donne en sortie : Un embedding avec des propriétés</p>
<ul>
<li><p>Fonctionne bien avec le machine learning</p></li>
<li><p>Les éléments similaires sont proche dans l’espace.</p></li>
<li><p>Garde à peu près la structure globales.</p></li>
</ul>
</li>
</ul>
</div>
<section id="deepwalk">
<h4>Deepwalk<a class="headerlink" href="#deepwalk" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line"><strong>Deepwalk</strong> est une méthode de <em>skipgram</em> pour les graphes.</div>
</div>
<ol class="arabic simple">
<li><p>Génère des <em>phrases</em> avec une marche aléatoire.</p></li>
<li><p>Applique <em>skipgram</em>.</p></li>
</ol>
</section>
<section id="node2vec">
<h4>Node2Vec<a class="headerlink" href="#node2vec" title="Lien permanent vers cette rubrique">#</a></h4>
<div class="line-block">
<div class="line"><strong>Node2Vec</strong> utilise une marche aléatoire biaisée pour tuné le contexte pour capturer <em>ceux que l’on souhaite</em>.</div>
</div>
</section>
</section>
<section id="roles-des-embeddings">
<h3>Rôles des embeddings<a class="headerlink" href="#roles-des-embeddings" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Dans <em>Node2Vec/Deepwalk</em>, le contexte collecté contient des <em>labels</em> des noeuds rencontrés. On pourrait mémoriser les <em>propriétés</em> des noeuds.</div>
</div>
</section>
<section id="deep-learning-and-embeddings">
<h3>Deep Learning and embeddings<a class="headerlink" href="#deep-learning-and-embeddings" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Après chaque couches de DNN, les objets sont représentés par les vecteurs. On peut ensuite utiliser ses embeddings pour d’autres tâches.</div>
</div>
</section>
<section id="objects-vectors-to-graphs">
<h3>Objects/Vectors to Graphs<a class="headerlink" href="#objects-vectors-to-graphs" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-approche-simple-matrice-de-correlation admonition">
<p class="admonition-title">Approche simple : Matrice de corrélation</p>
<ol class="arabic simple">
<li><p>Calculer la corrélation entre les variables.</p></li>
<li><p>Garder les corrélations au-dessus d’un threshold.</p></li>
<li><p>Les valeurs de corrélations peuvent être utilisées comme poids.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line">On peut utiliser les graphes comme alternative à la réduction de dimensions pour la visualisation.</div>
<div class="line">Dans beaucoup de cas, le réseau va être trop dense pour l’analyser. On peut utiliser une <em>backbone extraction</em> qui est une méthode qui retient les arrêtes importantes.</div>
</div>
</section>
</section>
<section id="frequent-pattern-mining">
<h2>Frequent Pattern Mining<a class="headerlink" href="#frequent-pattern-mining" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="line-block">
<div class="line">L’objectif est de trouver les objets qui apparaissent souvent ensemble.</div>
</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<ul class="simple">
<li><p><strong>Objets</strong> : <span class="math notranslate nohighlight">\(I={i_{1}, i_{2}, ..., i_{n}}\)</span></p></li>
<li><p><strong>Transaction</strong> : <span class="math notranslate nohighlight">\((t_{i} \subseteq I)\)</span></p></li>
<li><p><strong>Database</strong> : <span class="math notranslate nohighlight">\(D={t_{1}, t_{2}, ..., t_{m}}\)</span></p></li>
<li><p><strong>Itemset</strong> : <span class="math notranslate nohighlight">\(X \subseteq I\)</span></p></li>
<li><p><strong>Support absolu</strong> d’un itemset <span class="math notranslate nohighlight">\(X\)</span> in <span class="math notranslate nohighlight">\(D\)</span> : Nombre de transactions contenant <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><strong>Support relatif</strong> : <span class="math notranslate nohighlight">\(\frac{abs_support(X)}{|D|}\)</span></p></li>
<li><p><strong>Itemset fréquent</strong> : Itemset avec un support supérieur ou égal au plus petit support donné.</p></li>
<li><p><strong>Association rule</strong> : <span class="math notranslate nohighlight">\(X \rightarrow Y\)</span> avec <span class="math notranslate nohighlight">\(X,Y \subseteq I\)</span> et <span class="math notranslate nohighlight">\(X \cap Y = \emptyset\)</span>.</p></li>
<li><p><strong>Support</strong> de <span class="math notranslate nohighlight">\(X \rightarrow Y\)</span> : <span class="math notranslate nohighlight">\(W = X \cup Y\)</span></p></li>
</ul>
</div>
<section id="scores-d-interet">
<h3>Scores d’intérêt<a class="headerlink" href="#scores-d-interet" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="math notranslate nohighlight">
\[conf(X \Rightarrow Y) = P(Y|X) = \frac{supp(X \cap Y)}{supp(X)} = \frac{nombre \; de \; transactions \; contenant \; X \; et \; Y}{nombre \; de \; transactions \; contenant \; X}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>La confiance n’est pas symétrique.</p>
</div>
<div class="line-block">
<div class="line">Si <span class="math notranslate nohighlight">\(Y\)</span> à une grande confiance, mais est aussi fréquent, la confiance n’est pas suffisante.</div>
<div class="line">On utilise donc le <strong>lift</strong> : <span class="math notranslate nohighlight">\(lift(X \Rightarrow Y) = \frac{conf(X \Rightarrow Y)}{supp(Y)}\)</span>.</div>
</div>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(leverage(A \rightarrow C) = support (A \rightarrow C) - support(A) \times support(C)\)</span> : est la différence entre la fréquence observée de <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(C\)</span> apparaissant ensemble et la fréquence attendue si <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(C\)</span> étaient indépendants. 0 montre une indépendance.</div>
</div>
</section>
<section id="frequent-itemset-extraction">
<h3>Frequent Itemset Extraction<a class="headerlink" href="#frequent-itemset-extraction" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-approche-naive admonition">
<p class="admonition-title">Approche naïve</p>
<ol class="arabic simple">
<li><p>Générer tous les itemsets possibles.</p></li>
<li><p>Calculer le support de chaque itemset.</p></li>
</ol>
</div>
<div class="admonition warning">
<p class="admonition-title">Avertissement</p>
<div class="line-block">
<div class="line">Problème d’explosion combinatoire.</div>
</div>
</div>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(X_{1}\)</span> est fréquent, alors <span class="math notranslate nohighlight">\(X_{2} \subset X_{1}\)</span> est fréquent.</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(X_{1}\)</span> n’est pas fréquent, alors <span class="math notranslate nohighlight">\(X_{2}, X_{1} \subset X_{2}\)</span> n’est pas fréquent.</p></li>
</ul>
<div class="admonition-trick admonition">
<p class="admonition-title">Trick</p>
<ol class="arabic simple">
<li><p>Trouver les itemsets fréquents de taille 1.</p></li>
<li><p>Trouver les itemsets fréquents de taille 2, en utilisant que les itemsets fréquents de taille 1.</p></li>
<li><p>Répéter pour toutes les tailles.</p></li>
</ol>
</div>
<div class="line-block">
<div class="line">On définit un pattern <strong>closed</strong> comme un pattern fréquent, et un pattern <strong>maximal</strong> comme un pattern qui n’est pas un sous-pattern d’un autre pattern fréquent.</div>
</div>
</section>
<section id="algorithme-apriori">
<h3>Algorithme Apriori<a class="headerlink" href="#algorithme-apriori" title="Lien permanent vers cette rubrique">#</a></h3>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/dm-algoApriori.png"><img alt="_images/dm-algoApriori.png" src="_images/dm-algoApriori.png" style="width: 774.0px; height: 465.0px;" /></a>
</figure>
</section>
</section>
<section id="une-question-de-recherche-communities-in-degenerate-link-streams">
<h2>Une question de recherche : Communities in degenerate link streams<a class="headerlink" href="#une-question-de-recherche-communities-in-degenerate-link-streams" title="Lien permanent vers cette rubrique">#</a></h2>
<div class="line-block">
<div class="line">La plupart des réseaux sont dynamiques. Les noeuds et les liens apparaissent et disparaissent au cours du temps.</div>
</div>
<section id="slowly-evolving-networks-sen">
<h3>Slowly evolving networks (SEN)<a class="headerlink" href="#slowly-evolving-networks-sen" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Les arrêtes change lentement. Le réseau est bien définit à tout moment <span class="math notranslate nohighlight">\(t\)</span>.</div>
<div class="line">Un analyse statique à chaque temps <span class="math notranslate nohighlight">\(t\)</span> donne une vision dynamique.</div>
</div>
</section>
<section id="unstable-degenerate-temporal-networks">
<h3>Unstable/Degenerate temporal networks<a class="headerlink" href="#unstable-degenerate-temporal-networks" title="Lien permanent vers cette rubrique">#</a></h3>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="_images/dm-lsandig.png"><img alt="_images/dm-lsandig.png" src="_images/dm-lsandig.png" style="width: 711.0px; height: 361.0px;" /></a>
<figcaption>
<p><span class="caption-text">Interval Graph <em>(en haut)</em> et Link Stream <em>(en bas)</em></span><a class="headerlink" href="#id8" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<div class="line-block">
<div class="line">Une solution est de le transformer en SEN avec une aggregation/sliding window.</div>
</div>
</section>
<section id="centralites-propriete-d-un-reseau-dans-un-stream-graph">
<h3>Centralités &amp; Propriété d’un réseau dans un Stream Graph<a class="headerlink" href="#centralites-propriete-d-un-reseau-dans-un-stream-graph" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="admonition-definition admonition">
<p class="admonition-title">Définition</p>
<div class="line-block">
<div class="line">Le <strong>Stream Graph</strong> permet de représenter n’importe quel type de graphe dynamique.</div>
</div>
<div class="math notranslate nohighlight">
\[S = (T, V, W, E)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T\)</span> : L’ensemble des temps.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> : L’ensemble des noeuds.</p></li>
<li><p><span class="math notranslate nohighlight">\(W\)</span> : Les noeuds présent à un temps <span class="math notranslate nohighlight">\(V \times T\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(E\)</span> : Les liens présent à un temps <span class="math notranslate nohighlight">\(V \times V \times T\)</span>.</p></li>
</ul>
</div>
<div class="line-block">
<div class="line">Time-Entity designation :</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V_{t}\)</span> : L’ensemble des noeuds à un temps <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(E_{t}\)</span> : L’ensemble des liens à un temps <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(G_{t}\)</span> : Snapshot du graphe à un temps <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(G_{t} = (V_{t}, E_{t})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(v_{t}\)</span> : Existe si <span class="math notranslate nohighlight">\(v \in V_{t}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((u,v)_{t}\)</span> : Existe si <span class="math notranslate nohighlight">\((u,v) \in E_{t}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(T_{u}\)</span> : L’ensemble des temps où le noeud <span class="math notranslate nohighlight">\(u\)</span> existe.</p></li>
<li><p><span class="math notranslate nohighlight">\(T_{(u,v)}\)</span> : L’ensemble des temps où le lien <span class="math notranslate nohighlight">\((u,v)\)</span> existe.</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{u}\)</span> : La fraction du temps où le noeud <span class="math notranslate nohighlight">\(u\)</span> existe, <span class="math notranslate nohighlight">\(N_{u} = \frac{|T_{u}|}{|T|}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(L_{(u,v)}\)</span> : La fraction du temps où le lien <span class="math notranslate nohighlight">\((u,v)\)</span> existe, <span class="math notranslate nohighlight">\(L_{(u,v)} = \frac{|T_{(u,v)}|}{|T|}\)</span>.</p></li>
</ul>
<div class="line-block">
<div class="line">Redéfinition des notions de graphe :</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N = \sum_{v \in V} N_{v} = \frac{|W|}{|T|}\)</span> : Nombre de noeuds.</p></li>
<li><p><span class="math notranslate nohighlight">\(L = \sum_{(u,v) \in E} L_{(u,v)} = \frac{|E|}{|T|}\)</span> : Nombre de liens.</p></li>
<li><p><span class="math notranslate nohighlight">\(d = \frac{L}{L_{max}}\)</span> : Densité du graphe.</p></li>
</ul>
<div class="line-block">
<div class="line">Dans les SG, un cluster <span class="math notranslate nohighlight">\(C\)</span> est un sous-ensemble de <span class="math notranslate nohighlight">\(W\)</span>.</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">J’ai skip des notions parce que bon… définir des trucs et ne pas les utiliser ça suffit.</div>
</div>
</div>
</section>
<section id="random-models-for-dynamix-networks">
<h3>Random Models for dynamix networks<a class="headerlink" href="#random-models-for-dynamix-networks" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p><strong>Snapshot Shuffling</strong> : Garde l’ordre des snapshots, mais mélange les liens.</p></li>
<li><p><strong>Sequence Shuffling</strong> : Mélange les snapshots, mais garde les liens.</p></li>
<li><p><strong>Link Shuffling</strong> : Garde le temps d’activation des paires de noeuds, rend aléatoire le graphe aggréger.</p></li>
<li><p><strong>Timeline Shuffling</strong> : Garde le graphe aggréger, mais rend aléatoire le temps d’activation des paires de noeuds.</p></li>
</ul>
</section>
<section id="dynamic-community-detection">
<h3>Dynamic Community Detection<a class="headerlink" href="#dynamic-community-detection" title="Lien permanent vers cette rubrique">#</a></h3>
<div class="line-block">
<div class="line">Différentes approches :</div>
</div>
<ul class="simple">
<li><p><strong>Instant optimal</strong> :</p>
<ul>
<li><p>Permet de réutiliser des algorithmes statiques.</p></li>
<li><p>Pas de <em>partition smoothing</em>.</p></li>
<li><p>Les labels peuvent être <em>smoothed</em>.</p></li>
<li><p>Simple à parallèliser.</p></li>
</ul>
</li>
<li><p><strong>Temporal trade-off</strong> :</p>
<ul>
<li><p>Ne peut pas être parallelisé (itératif).</p></li>
<li><p>Bien pour les analyses en temps réels.</p></li>
</ul>
</li>
<li><p><strong>Croos-Time</strong> :</p>
<ul>
<li><p>Demande de connaître l’évolution entière en avance.</p></li>
<li><p>Bien pour les interprétation à postériori.</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><strong>No Smoothness</strong> : Aucune partition à l’instant <span class="math notranslate nohighlight">\(t\)</span> doit être la même que celle trouvé par un algorithme statique.</p></li>
<li><p><strong>Smoothness</strong> : Une partition à l’instant <span class="math notranslate nohighlight">\(t\)</span> est un compromis entre les <em>bonnes</em> communautés d’un graphe à l’instant <span class="math notranslate nohighlight">\(t\)</span> et les similitudes avec des partitions à différent moments.</p></li>
</ul>
</div>
<div class="line-block">
<div class="line"><em>Puis il parle de sur quoi il bosse atm donc c’est pas grave.</em></div>
</div>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="taa.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">TAA - Techniques d’apprentissage automatique</p>
      </div>
    </a>
    <a class="right-next"
       href="biml.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">BIML - Bio-Inspired Machine Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-data-description">Introduction, Data Description</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-de-donnees">Types de données</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decrire-une-variable">Décrire une variable</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value">P-value</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deviation-standard">Déviation standard</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions-de-variables">Interactions de variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrice-de-covariance">Matrice de covariance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficient-de-correlation">Coefficient de corrélation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-de-spearman">Corrélation de Spearman</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notions-de-distances">Notions de distances</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature scaling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regles-d-or">Règles d’or</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-beyond-k-means">Clustering beyond k-means</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means">K-means</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#melanges-gaussien-gaussian-mixtures">Mélanges Gaussien (Gaussian mixtures)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan">DBSCAN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-de-clustering">Evaluation de clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#score-silhouette">Score silhouette</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#davies-bouldin-index-dbi">Davies-Bouldin Index (DBI)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dunn-index">Dunn Index</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-i-centralities">Networks I - Centralities</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#centralitees">Centralitées</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions-recursives">Définitions récursives</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank">PageRank</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-ii-detection-de-communautes">Networks II - Détection de communautés</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quelques-algos">Quelques algos</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#girvan-newman">Girvan &amp; Newman</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#louvain">Louvain</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#infomap">Infomap</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-block-model-sbm">Stochastic Block Model (SBM)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-de-structure-de-communautes">Evaluation de structure de communautés</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-de-liens">Prédiction de liens</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#premiere-approche-non-supervise-heuristique">Première approche (Non-supervisé) - Heuristique</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#seconde-approche-supervise-score-de-similarite">Seconde approche (Supervisé) - Score de similarité</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-de-noeuds">Classification de noeuds</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#methodes-recentes">Méthodes récentes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendation-factorisation-matricielle">Recommendation / Factorisation matricielle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-du-modele">Définition du modèle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-based-knn">User-based KNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#item-based-collaborative-filtering">Item-based Collaborative Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization-collaborative-filtering">Matrix Factorization Collaborative Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-des-systemes-de-recommandations">Evaluation des systèmes de recommandations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variante-de-mf-non-negative-matrix-factorization-nmf">Variante de MF - Non-negative Matrix Factorization (NMF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#co-clustering">Co-Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-de-dimension-low-dimensionality-embedding">Reduction de dimension <em>Low dimensionality embedding</em></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manifolds">Manifolds</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mds">MDS</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#isomap">Isomap</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-sne">T-SNE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-dimensionality-embedding">Low dimensionality embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding">Word Embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-embedding">Graph Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deepwalk">Deepwalk</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roles-des-embeddings">Rôles des embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-and-embeddings">Deep Learning and embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objects-vectors-to-graphs">Objects/Vectors to Graphs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequent-pattern-mining">Frequent Pattern Mining</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scores-d-interet">Scores d’intérêt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequent-itemset-extraction">Frequent Itemset Extraction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme-apriori">Algorithme Apriori</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#une-question-de-recherche-communities-in-degenerate-link-streams">Une question de recherche : Communities in degenerate link streams</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slowly-evolving-networks-sen">Slowly evolving networks (SEN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unstable-degenerate-temporal-networks">Unstable/Degenerate temporal networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#centralites-propriete-d-un-reseau-dans-un-stream-graph">Centralités &amp; Propriété d’un réseau dans un Stream Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-models-for-dynamix-networks">Random Models for dynamix networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-community-detection">Dynamic Community Detection</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Axel
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Axel.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>